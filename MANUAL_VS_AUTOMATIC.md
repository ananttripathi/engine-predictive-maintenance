# Manual vs Automatic Uploads - Quick Guide

## ðŸš€ AUTOMATIC Uploads (Via Scripts)

These happen automatically when you run the scripts. **You don't need to manually upload anything to Hugging Face.**

### âœ… Automatic: Hugging Face Dataset Repo
**Script**: `python src/data_register.py` and `python src/data_prep.py`

**What gets uploaded automatically:**
- âœ… `data/engine_data.csv` â†’ HF Dataset Repo
- âœ… `data/processed/train.csv` â†’ HF Dataset Repo  
- âœ… `data/processed/test.csv` â†’ HF Dataset Repo

**You just run:**
```bash
python src/data_register.py    # Auto-uploads raw data
python src/data_prep.py        # Auto-uploads train/test
```

**That's it!** The scripts handle everything - creating the repo, uploading files, etc.

---

### âœ… Automatic: Hugging Face Model Repo
**Script**: `python src/train.py`

**What gets uploaded automatically:**
- âœ… `models/best_model.joblib` â†’ HF Model Repo (as `model.joblib`)

**You just run:**
```bash
python src/train.py
```

**That's it!** The script:
1. Trains the model
2. Saves it locally
3. Automatically uploads to HF Model Repo

---

### âœ… Automatic: Hugging Face Space (Streamlit App)
**Script**: `python src/deploy_to_hf.py`

**What gets uploaded automatically:**
- âœ… `src/app.py` (Streamlit app)
- âœ… `src/inference.py`
- âœ… `src/config.py`
- âœ… `Dockerfile`
- âœ… `requirements.txt`
- âœ… Other `src/*.py` files needed

**You just run:**
```bash
python src/deploy_to_hf.py
```

**That's it!** The script:
1. Creates/updates the HF Space
2. Uploads all deployment files
3. Configures it as a Streamlit app

---

## ðŸ“¤ MANUAL Uploads (You Do This)

Only **ONE** thing needs to be done manually:

### âš ï¸ Manual: GitHub Repository

**What you need to do manually:**
- Push your entire `mlops/` folder to GitHub

**Steps:**
```bash
cd /Users/ananttripathi/Desktop/mlops

# 1. Initialize git (if not done)
git init

# 2. Create .gitignore (to exclude large files)
cat > .gitignore << EOF
.venv/
__pycache__/
*.pyc
mlruns/
*.log
.DS_Store
EOF

# 3. Add all files
git add .

# 4. Commit
git commit -m "Initial commit: Predictive Maintenance MLOps Pipeline"

# 5. Add your GitHub repo as remote
git remote add origin https://github.com/ananttripathi/engine-predictive-maintenance.git

# 6. Push to GitHub
git push -u origin main
```

**That's the ONLY manual upload!**

---

## ðŸ“Š Summary Table

| Destination | Upload Method | What You Do |
|------------|---------------|-------------|
| **GitHub** | âš ï¸ **MANUAL** | Run `git push` commands |
| **HF Dataset** | âœ… **AUTOMATIC** | Run `python src/data_register.py` and `python src/data_prep.py` |
| **HF Model** | âœ… **AUTOMATIC** | Run `python src/train.py` |
| **HF Space** | âœ… **AUTOMATIC** | Run `python src/deploy_to_hf.py` |

---

## ðŸŽ¯ Complete Workflow

### Step 1: Manual - GitHub (Do This First)
```bash
# Push everything to GitHub
git init
git add .
git commit -m "Initial commit"
git remote add origin https://github.com/ananttripathi/engine-predictive-maintenance.git
git push -u origin main
```
â±ï¸ **Time**: 2-3 minutes

---

### Step 2: Automatic - HF Dataset
```bash
export HF_TOKEN="your_hf_token"
python src/data_register.py
python src/data_prep.py
```
â±ï¸ **Time**: 1-2 minutes (scripts do everything)

---

### Step 3: Automatic - HF Model
```bash
python src/train.py
```
â±ï¸ **Time**: 5-10 minutes (trains model + auto-uploads)

---

### Step 4: Automatic - HF Space
```bash
python src/deploy_to_hf.py
```
â±ï¸ **Time**: 1-2 minutes (scripts do everything)

---

## âœ… What You Need to Prepare

### Before Running Scripts:

1. **Set Environment Variable:**
   ```bash
   export HF_TOKEN="hf_your_token_here"
   ```

2. **Update `src/config.py`** (already done):
   - HF repos are set to `ananttripathiak/...`

3. **That's it!** Scripts handle the rest automatically.

---

## ðŸ” Verification

After running scripts, verify automatic uploads:

1. **HF Dataset**: https://huggingface.co/datasets/ananttripathiak/engine-maintenance-dataset
   - Should see: `data/engine_data.csv`, `data/train.csv`, `data/test.csv`

2. **HF Model**: https://huggingface.co/ananttripathiak/engine-maintenance-model
   - Should see: `model.joblib`

3. **HF Space**: https://huggingface.co/spaces/ananttripathiak/engine-maintenance-space
   - Should see: `src/app.py`, `Dockerfile`, `requirements.txt`
   - App should be running!

---

## ðŸ’¡ Key Points

âœ… **Automatic (3 out of 4):**
- HF Dataset upload
- HF Model upload  
- HF Space deployment

âš ï¸ **Manual (1 out of 4):**
- GitHub push (only this one!)

ðŸŽ¯ **Bottom Line:**
- **75% automatic** - Just run the scripts!
- **25% manual** - Just push to GitHub once!

---

## ðŸš¨ Common Questions

**Q: Do I need to manually create HF repos?**
A: **No!** Scripts create them automatically if they don't exist.

**Q: Do I need to manually upload files to HF?**
A: **No!** Scripts upload everything automatically.

**Q: What if a script fails?**
A: Check `HF_TOKEN` is set, then re-run the script. It will continue from where it left off.

**Q: Can I skip GitHub and just use HF?**
A: For the project rubric, you need GitHub for the CI/CD workflow. But for HF-only, yes, scripts handle everything.

---

## ðŸ“ Quick Checklist

- [ ] **Manual**: Push to GitHub (`git push`)
- [ ] **Automatic**: Run `python src/data_register.py`
- [ ] **Automatic**: Run `python src/data_prep.py`
- [ ] **Automatic**: Run `python src/train.py`
- [ ] **Automatic**: Run `python src/deploy_to_hf.py`
- [ ] Verify all uploads completed successfully

**Total Manual Work: ~5 minutes (just GitHub)**
**Total Automatic Work: ~10-15 minutes (scripts do everything)**
